// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/ai/generativelanguage/v1beta3/discuss_service.proto

package com.google.ai.generativelanguage.v1beta3;

public interface MessagePromptOrBuilder extends
    // @@protoc_insertion_point(interface_extends:google.ai.generativelanguage.v1beta3.MessagePrompt)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * Optional. Text that should be provided to the model first to ground the
   * response.
   *
   * If not empty, this `context` will be given to the model first before the
   * `examples` and `messages`. When using a `context` be sure to provide it
   * with every request to maintain continuity.
   *
   * This field can be a description of your prompt to the model to help provide
   * context and guide the responses. Examples: "Translate the phrase from
   * English to French." or "Given a statement, classify the sentiment as happy,
   * sad or neutral."
   *
   * Anything included in this field will take precedence over message history
   * if the total input size exceeds the model's `input_token_limit` and the
   * input request is truncated.
   * </pre>
   *
   * <code>string context = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return The context.
   */
  java.lang.String getContext();
  /**
   * <pre>
   * Optional. Text that should be provided to the model first to ground the
   * response.
   *
   * If not empty, this `context` will be given to the model first before the
   * `examples` and `messages`. When using a `context` be sure to provide it
   * with every request to maintain continuity.
   *
   * This field can be a description of your prompt to the model to help provide
   * context and guide the responses. Examples: "Translate the phrase from
   * English to French." or "Given a statement, classify the sentiment as happy,
   * sad or neutral."
   *
   * Anything included in this field will take precedence over message history
   * if the total input size exceeds the model's `input_token_limit` and the
   * input request is truncated.
   * </pre>
   *
   * <code>string context = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return The bytes for context.
   */
  com.google.protobuf.ByteString
      getContextBytes();

  /**
   * <pre>
   * Optional. Examples of what the model should generate.
   *
   * This includes both user input and the response that the model should
   * emulate.
   *
   * These `examples` are treated identically to conversation messages except
   * that they take precedence over the history in `messages`:
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated. Items will be dropped from `messages` before `examples`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Example examples = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   */
  java.util.List<com.google.ai.generativelanguage.v1beta3.Example> 
      getExamplesList();
  /**
   * <pre>
   * Optional. Examples of what the model should generate.
   *
   * This includes both user input and the response that the model should
   * emulate.
   *
   * These `examples` are treated identically to conversation messages except
   * that they take precedence over the history in `messages`:
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated. Items will be dropped from `messages` before `examples`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Example examples = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   */
  com.google.ai.generativelanguage.v1beta3.Example getExamples(int index);
  /**
   * <pre>
   * Optional. Examples of what the model should generate.
   *
   * This includes both user input and the response that the model should
   * emulate.
   *
   * These `examples` are treated identically to conversation messages except
   * that they take precedence over the history in `messages`:
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated. Items will be dropped from `messages` before `examples`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Example examples = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   */
  int getExamplesCount();
  /**
   * <pre>
   * Optional. Examples of what the model should generate.
   *
   * This includes both user input and the response that the model should
   * emulate.
   *
   * These `examples` are treated identically to conversation messages except
   * that they take precedence over the history in `messages`:
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated. Items will be dropped from `messages` before `examples`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Example examples = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   */
  java.util.List<? extends com.google.ai.generativelanguage.v1beta3.ExampleOrBuilder> 
      getExamplesOrBuilderList();
  /**
   * <pre>
   * Optional. Examples of what the model should generate.
   *
   * This includes both user input and the response that the model should
   * emulate.
   *
   * These `examples` are treated identically to conversation messages except
   * that they take precedence over the history in `messages`:
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated. Items will be dropped from `messages` before `examples`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Example examples = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
   */
  com.google.ai.generativelanguage.v1beta3.ExampleOrBuilder getExamplesOrBuilder(
      int index);

  /**
   * <pre>
   * Required. A snapshot of the recent conversation history sorted
   * chronologically.
   *
   * Turns alternate between two authors.
   *
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated: The oldest items will be dropped from `messages`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Message messages = 3 [(.google.api.field_behavior) = REQUIRED];</code>
   */
  java.util.List<com.google.ai.generativelanguage.v1beta3.Message> 
      getMessagesList();
  /**
   * <pre>
   * Required. A snapshot of the recent conversation history sorted
   * chronologically.
   *
   * Turns alternate between two authors.
   *
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated: The oldest items will be dropped from `messages`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Message messages = 3 [(.google.api.field_behavior) = REQUIRED];</code>
   */
  com.google.ai.generativelanguage.v1beta3.Message getMessages(int index);
  /**
   * <pre>
   * Required. A snapshot of the recent conversation history sorted
   * chronologically.
   *
   * Turns alternate between two authors.
   *
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated: The oldest items will be dropped from `messages`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Message messages = 3 [(.google.api.field_behavior) = REQUIRED];</code>
   */
  int getMessagesCount();
  /**
   * <pre>
   * Required. A snapshot of the recent conversation history sorted
   * chronologically.
   *
   * Turns alternate between two authors.
   *
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated: The oldest items will be dropped from `messages`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Message messages = 3 [(.google.api.field_behavior) = REQUIRED];</code>
   */
  java.util.List<? extends com.google.ai.generativelanguage.v1beta3.MessageOrBuilder> 
      getMessagesOrBuilderList();
  /**
   * <pre>
   * Required. A snapshot of the recent conversation history sorted
   * chronologically.
   *
   * Turns alternate between two authors.
   *
   * If the total input size exceeds the model's `input_token_limit` the input
   * will be truncated: The oldest items will be dropped from `messages`.
   * </pre>
   *
   * <code>repeated .google.ai.generativelanguage.v1beta3.Message messages = 3 [(.google.api.field_behavior) = REQUIRED];</code>
   */
  com.google.ai.generativelanguage.v1beta3.MessageOrBuilder getMessagesOrBuilder(
      int index);
}
