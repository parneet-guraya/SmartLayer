// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/ai/generativelanguage/v1beta3/discuss_service.proto

package com.google.ai.generativelanguage.v1beta3;

public interface GenerateMessageRequestOrBuilder extends
    // @@protoc_insertion_point(interface_extends:google.ai.generativelanguage.v1beta3.GenerateMessageRequest)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * Required. The name of the model to use.
   *
   * Format: `name=models/{model}`.
   * </pre>
   *
   * <code>string model = 1 [(.google.api.field_behavior) = REQUIRED, (.google.api.resource_reference) = { ... }</code>
   * @return The model.
   */
  java.lang.String getModel();
  /**
   * <pre>
   * Required. The name of the model to use.
   *
   * Format: `name=models/{model}`.
   * </pre>
   *
   * <code>string model = 1 [(.google.api.field_behavior) = REQUIRED, (.google.api.resource_reference) = { ... }</code>
   * @return The bytes for model.
   */
  com.google.protobuf.ByteString
      getModelBytes();

  /**
   * <pre>
   * Required. The structured textual input given to the model as a prompt.
   *
   * Given a
   * prompt, the model will return what it predicts is the next message in the
   * discussion.
   * </pre>
   *
   * <code>.google.ai.generativelanguage.v1beta3.MessagePrompt prompt = 2 [(.google.api.field_behavior) = REQUIRED];</code>
   * @return Whether the prompt field is set.
   */
  boolean hasPrompt();
  /**
   * <pre>
   * Required. The structured textual input given to the model as a prompt.
   *
   * Given a
   * prompt, the model will return what it predicts is the next message in the
   * discussion.
   * </pre>
   *
   * <code>.google.ai.generativelanguage.v1beta3.MessagePrompt prompt = 2 [(.google.api.field_behavior) = REQUIRED];</code>
   * @return The prompt.
   */
  com.google.ai.generativelanguage.v1beta3.MessagePrompt getPrompt();
  /**
   * <pre>
   * Required. The structured textual input given to the model as a prompt.
   *
   * Given a
   * prompt, the model will return what it predicts is the next message in the
   * discussion.
   * </pre>
   *
   * <code>.google.ai.generativelanguage.v1beta3.MessagePrompt prompt = 2 [(.google.api.field_behavior) = REQUIRED];</code>
   */
  com.google.ai.generativelanguage.v1beta3.MessagePromptOrBuilder getPromptOrBuilder();

  /**
   * <pre>
   * Optional. Controls the randomness of the output.
   *
   * Values can range over `[0.0,1.0]`,
   * inclusive. A value closer to `1.0` will produce responses that are more
   * varied, while a value closer to `0.0` will typically result in
   * less surprising responses from the model.
   * </pre>
   *
   * <code>optional float temperature = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return Whether the temperature field is set.
   */
  boolean hasTemperature();
  /**
   * <pre>
   * Optional. Controls the randomness of the output.
   *
   * Values can range over `[0.0,1.0]`,
   * inclusive. A value closer to `1.0` will produce responses that are more
   * varied, while a value closer to `0.0` will typically result in
   * less surprising responses from the model.
   * </pre>
   *
   * <code>optional float temperature = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return The temperature.
   */
  float getTemperature();

  /**
   * <pre>
   * Optional. The number of generated response messages to return.
   *
   * This value must be between
   * `[1, 8]`, inclusive. If unset, this will default to `1`.
   * </pre>
   *
   * <code>optional int32 candidate_count = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return Whether the candidateCount field is set.
   */
  boolean hasCandidateCount();
  /**
   * <pre>
   * Optional. The number of generated response messages to return.
   *
   * This value must be between
   * `[1, 8]`, inclusive. If unset, this will default to `1`.
   * </pre>
   *
   * <code>optional int32 candidate_count = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return The candidateCount.
   */
  int getCandidateCount();

  /**
   * <pre>
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Nucleus sampling considers the smallest set of tokens whose probability
   * sum is at least `top_p`.
   * </pre>
   *
   * <code>optional float top_p = 5 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return Whether the topP field is set.
   */
  boolean hasTopP();
  /**
   * <pre>
   * Optional. The maximum cumulative probability of tokens to consider when
   * sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Nucleus sampling considers the smallest set of tokens whose probability
   * sum is at least `top_p`.
   * </pre>
   *
   * <code>optional float top_p = 5 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return The topP.
   */
  float getTopP();

  /**
   * <pre>
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   * </pre>
   *
   * <code>optional int32 top_k = 6 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return Whether the topK field is set.
   */
  boolean hasTopK();
  /**
   * <pre>
   * Optional. The maximum number of tokens to consider when sampling.
   *
   * The model uses combined Top-k and nucleus sampling.
   *
   * Top-k sampling considers the set of `top_k` most probable tokens.
   * </pre>
   *
   * <code>optional int32 top_k = 6 [(.google.api.field_behavior) = OPTIONAL];</code>
   * @return The topK.
   */
  int getTopK();
}
